import { GoogleGenAI } from "@google/genai";
import { Era } from '../types';

export const transformImage = async (base64Image: string, era: Era): Promise<string> => {
  if (!process.env.API_KEY) {
    throw new Error("API Key is missing. Please set the API_KEY environment variable.");
  }

  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
  
  // Clean base64 string if it contains metadata header
  const cleanBase64 = base64Image.replace(/^data:image\/(png|jpeg|jpg|webp);base64,/, '');

  try {
    // We use gemini-2.5-flash-image for image-to-image/editing capabilities
    // This model is efficient for transformations.
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: {
        parts: [
          {
            inlineData: {
              data: cleanBase64,
              mimeType: 'image/jpeg', // Standardizing on JPEG for input
            },
          },
          {
            text: `Transform the person in this image into a ${era.name} character. 
            Style guide: ${era.promptModifier}. 
            Maintain the person's facial structure, gender, and pose, but completely change the clothing, accessories, and background to match the era. 
            High quality, detailed, cinematic.`
          },
        ],
      },
      // Note: responseMimeType is not supported for vision models in some contexts, 
      // but we expect text/image parts back. 
    });

    // Parse response for image
    const candidates = response.candidates;
    if (candidates && candidates.length > 0) {
      for (const part of candidates[0].content.parts) {
        if (part.inlineData) {
          return `data:image/png;base64,${part.inlineData.data}`;
        }
      }
    }
    
    throw new Error("No image generated by the model.");

  } catch (error) {
    console.error("Gemini API Error:", error);
    throw error;
  }
};
